\chapter{Summary}
\label{chapter:summary}

This dissertation is a compilation of three different data-driven experimental studies into intergroup bias in natural language online.

\paragraph{Intergroup bias in tweets} Chapter~\ref{chapter:twitter} introduced the idea of studying intergroup bias more broadly as distinct from the artificial conditions under which the LIB has proven to exist. By decomposing the bias into two constituent dimensions --- emotion and intergroup relationship, we can observe systematic relationships between these two. Modeling further fortifies this correlation, showing that one can derive insights into intergroup bias through modeling and analysis of natural language use online.

\paragraph{Counterfactual probing for intergroup bias} Building atop the dataset and models in Chapter~\ref{chapter:twitter}, in Chapter~\ref{chapter:probing} we investigated if the neural models fine-tuned for predicting intergroup relationships use human-interpretable higher order semantic features like affect and specificity in making their decisions. Counterfactual probing techniques allowed us to ask this question in a measured way on neural models. They demonstrated that these individual features act in measurable, intuitive ways on neural model prediction of intergroup relationship over an utterance, but we failed to find an interaction between these two features as hypothesized. These experiments revealed the need for grounded language data in order to tie systematic variation in in-group and out-group speech to events that preceded (or precipitated) them.

\paragraph{Grounding intergroup bias in football comments} Chapter~\ref{chapter:football} introduces a new dataset of interpersonal comments from live game threads on NFL team subreddits. Using statistics from the live game happening in parallel to the discussion on a thread, we can use win probability as a well calibrated appraisal of state-of-the-world leading up to comments by participants about the in-group or out-group. We find several significant, linear patterns in the form of referent used to refer to in-group and out-group; The more likely the in-group is to win, the more likely fans are to abstract away from referring to the in-group by name, pronoun or even implicitly, towards abstract descriptions of the events with no clear referent. Referents to the out-group however, increase as in-group win probability goes up. Overall, this chapter showed how careful use of LLMs to obtain comments tagged with intergroup labels at scale, enabled us to discover systematic variations in the \emph{form of referent} towards the in/out-group, a form of variation not hypothesized in the original LIB hypothesis.

Overall, the findings in this dissertation refine the LIB for broad application and study in real-world language use, by encompassing more forms of variation in the entirety of an utterance, and including other axes of human expression, like affect and emotion. The findings in this dissertation should encourage researchers of social bias in NLP (and computational studies of language/social science at large) to center the relationship between social structure and language use towards bias detection and mitigation --- the absence of which can overlook the many ways bias can seep into everyday communication.

\section{Future Work}

This dissertation is an attempt at answering complex questions about subtle changes in linguistic behavior. There are several unanswered questions, and promising topics for future investigation, a few of which I summarize below.

\paragraph{Stereotypes} The LIB hypothesis is a mechanism for how bias subconsciously seeps into utterances; However, the impetus for the hypothesis was to explain how stereotypes persist and propagate in communication. This was the reason for the focus on \emph{abstractness} of predicates --- there is some evidence that `abstract' language (as narrowly defined) is liable to reinforce conventions and labels cognitively. By relaxing the focus on stereotype-related behaviors and speech, we have been able to study more natural language; But this renews the question of what effect the observed linguistic behaviors have on stereotype perpetuation.

\paragraph{Parallel \& Multilingual} Chapter~\ref{chapter:football} introduces a dataset of interpersonal comments, with parallel live-game statistics to ground the utterances. Beyond the findings in \textsection~\ref{sec:football-analysis}, the dataset has a wealth of information in the \emph{parallel} nature of the utterances from fans of both teams observing the same sequence of events. Furthermore, there are possibilities for expanding the scope of this study to multilingual parallel comments, for instance from the live-broadcast comments of NFL games in Spanish and English. 

\paragraph{Language Modeling with grounding} As the findings in Chapter~\ref{chapter:football} reveal, win probability (WP) is a remarkable real-valued indicator of the state-of-the-world, compressed succinctly in a real-valued number. Crucially, it grounds the utterance for both sets of fans and their in-group robustly, evidenced by the linear relationship between WP and several linguistic indicators. While a wealth of work has looked into grounding language models in images and other modalities~\citep{Radford2021LearningTV}, there has been relatively little work looking into grounding with parallel meaning representations of ground state. The parallel corpus of game comments from two perspectives, with a non-linguistic description of events, should prove to be an exciting avenue for research into training language models with steerable intergroup dynamics, or perspective.
